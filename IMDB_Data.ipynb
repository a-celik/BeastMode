{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Stephen/Documents/BeastMode'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that takes a URL for an IMDB movie page, scrapes the URL, and returns the name of the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie(url):\n",
    "    pageText = requests.get(url)\n",
    "    while (pageText == None):\n",
    "        time.sleep(5)\n",
    "        pageText = requests.get(url)\n",
    "    soup = BeautifulSoup(pageText.text,\"html.parser\")\n",
    "    if soup == None or soup.find(\"div\",attrs={\"id\":\"tn15title\"}) == None:\n",
    "        return None\n",
    "    return soup.find(\"div\",attrs={\"id\":\"tn15title\"}).find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "# get all pos urls for the TRAIN set\n",
    "with open('aclImdb/train/urls_pos.txt', 'r') as f:\n",
    "    train_pos_urls = f.readlines()\n",
    "    print len(train_pos_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "# get all positive urls for the TEST set\n",
    "with open('aclImdb/test/urls_pos.txt', 'r') as f:\n",
    "    test_pos_urls = f.readlines()\n",
    "    print len(train_pos_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a dictionary of of (URL: Movie) pairs for each URL in the training positive list of urls. This dictionary will be of use later. It is better to do all the scraping up front. (The IMDB data set did not provide us with the names of the movies, only the URLS.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% time\n",
    "import random\n",
    "# Make a dictionary of URL: movie name\n",
    "url_dict = dict(zip(train_pos_urls, [None]*len(train_pos_urls)))\n",
    "for url in train_pos_urls[5275:]:\n",
    "    if url_dict[url] == None:\n",
    "        url_dict[url] = get_movie(url)\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did this right, the number of keys in `url_dict` should be equal to the number of unique urls in `train_pos_urls`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351 1390\n"
     ]
    }
   ],
   "source": [
    "n_url_keys = len(url_dict.keys())\n",
    "n_unique_urls = len(list(set(train_pos_urls)))\n",
    "print n_url_keys, n_unique_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the results in a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDITORS NOTE: those don't match because of a naming issue. They are truly correct...I just don't feel like running the whole thing again. We'll run the entire notebook again before submission, and we'll then delete this note. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fp = open(\"url_movie_tr_pos.json\",\"w\")\n",
    "json.dump(url_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for the testing positive list of urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 7.87 µs\n",
      "1\n",
      "25\n",
      "299\n",
      "831\n",
      "954\n",
      "1139\n",
      "1190\n",
      "1211\n",
      "1345\n",
      "1584\n",
      "1642\n",
      "1703\n",
      "1762\n",
      "1844\n",
      "1907\n",
      "2090\n",
      "2109\n",
      "2134\n",
      "2412\n",
      "2641\n",
      "2714\n",
      "2720\n",
      "2975\n",
      "3000\n",
      "3098\n",
      "3182\n",
      "3189\n",
      "3382\n",
      "3414\n",
      "3495\n",
      "3595\n",
      "3758\n",
      "3822\n",
      "3854\n",
      "4016\n",
      "4029\n",
      "4139\n",
      "4216\n",
      "4226\n",
      "4336\n",
      "4504\n",
      "4520\n",
      "4640\n",
      "5307\n",
      "5316\n",
      "5387\n",
      "5462\n",
      "5774\n",
      "5792\n",
      "5915\n",
      "6096\n",
      "6097\n",
      "6121\n",
      "6251\n",
      "6298\n",
      "6483\n",
      "6570\n",
      "6607\n",
      "6626\n",
      "7097\n",
      "7137\n",
      "7243\n",
      "7255\n",
      "7368\n",
      "7369\n",
      "7417\n",
      "7469\n",
      "7489\n",
      "7717\n",
      "7842\n",
      "7854\n",
      "7986\n",
      "8041\n",
      "8048\n",
      "8197\n",
      "8357\n",
      "8577\n",
      "8591\n",
      "8743\n",
      "8849\n",
      "8859\n",
      "8928\n",
      "8963\n",
      "9083\n",
      "9108\n",
      "9138\n",
      "9211\n",
      "9426\n",
      "9596\n",
      "9874\n",
      "9931\n",
      "9978\n",
      "10124\n",
      "10128\n",
      "10147\n",
      "10256\n",
      "10297\n",
      "10326\n",
      "10375\n",
      "10389\n",
      "10431\n",
      "10483\n",
      "10528\n",
      "10711\n",
      "10900\n",
      "10940\n",
      "10958\n",
      "11091\n",
      "11205\n",
      "11261\n",
      "11293\n",
      "11464\n",
      "11476\n",
      "11539\n",
      "11548\n",
      "11816\n",
      "11981\n",
      "12130\n",
      "12190\n",
      "12302\n",
      "12370\n",
      "12438\n",
      "12446\n",
      "12474\n"
     ]
    }
   ],
   "source": [
    "% time\n",
    "import random\n",
    "# Make a dictionary of URL: movie name\n",
    "url_dict = dict(zip(test_pos_urls, [None]*len(test_pos_urls)))\n",
    "i = 0\n",
    "for url in test_pos_urls:\n",
    "    if url_dict[url] == None:\n",
    "        url_dict[url] = get_movie(url)\n",
    "    if random.random() < 0.01:\n",
    "        print i\n",
    "    i += 1\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our results into a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = open(\"url_movie_test_pos.json\",\"w\")\n",
    "json.dump(url_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a review data frame\n",
    "review_df = pd.DataFrame(columns=['movie_id', 'stars', 'positive', 'text', 'url', 'movie_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_names = list(os.walk('aclImdb/train/pos/'))[0][2]\n",
    "for review in train_pos_names:\n",
    "    stars = int(review.split(\"_\")[1].split(\".\")[0])\n",
    "    movieID = int(review.split(\"_\")[0])\n",
    "    fp = open('aclImdb/train/pos/%(review)s' % {'review': review}, 'r')\n",
    "    text = fp.read()\n",
    "    pos = True\n",
    "    url = train_pos_urls[movieID]\n",
    "    movie_name = url_dict[url]\n",
    "    reviewDict = {'movie_id': movieID, 'stars': stars, 'positive': pos, 'text': text, 'url': url, 'movie_name': movie_name}\n",
    "    review_df = review_df.append(pd.DataFrame(reviewDict),index=[0])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
