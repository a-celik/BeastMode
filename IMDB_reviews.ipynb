{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109: IMDB review data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gathered data from Andrew L. Maas from Standford University, based on IMDB movies review (describe this more). The data was automatically split into test and train sets, with each set containing polarized movie reviews (each review was a text file) in subdirectories. Since the creators of the original data set were not interested in predicted Box Office Scores, they didn't bother to save the names of the movies, only the URLs that the reviews were scraped from on IMDB.com. Thus we had to go back into all those URLs and scrape the movie names from the top of the pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, '/aclImdb/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we write a function to scrape an IMDB url and return a movie name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to get name of movie from each URL\n",
    "def get_movie(url):\n",
    "    '''\n",
    "    Scrapes a given URL from IMDB.com. The URL's page contains many reviews for one particular movie.\n",
    "    This function returns the name of that  movie. \n",
    "    '''\n",
    "    pageText = requests.get(url)\n",
    "    # Keep asking for the page until you get it. Sleep if necessary.\n",
    "    while (pageText==None):\n",
    "        time.sleep(5)\n",
    "        pageText = requests.get(url)\n",
    "    soup = BeautifulSoup(pageText.text,\"html.parser\")\n",
    "    # Some of our URL's are expired! Return None if so.\n",
    "    if soup == None or soup.find(\"div\",attrs={\"id\":\"tn15title\"}) == None:\n",
    "        return None\n",
    "    return soup.find(\"div\",attrs={\"id\":\"tn15title\"}).find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the list of URLs for each of our data sets: both positive and negative for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all urls for train and test, neg and pos\n",
    "with open('aclImdb/train/urls_pos.txt','r') as f:\n",
    "    train_pos_urls = f.readlines()\n",
    "    \n",
    "with open('aclImdb/train/urls_neg.txt','r') as f:\n",
    "    train_neg_urls = f.readlines()\n",
    "\n",
    "with open('aclImdb/test/urls_pos.txt','r') as f:\n",
    "    test_pos_urls = f.readlines()\n",
    "    \n",
    "with open('aclImdb/test/urls_neg.txt','r') as f:\n",
    "    test_neg_urls = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long each list is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 12500 12500 12500\n"
     ]
    }
   ],
   "source": [
    "print len(train_pos_urls), len(train_neg_urls), len(test_pos_urls), len(test_neg_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12500 reviews in each sub data set. Each review has a corresponding URL. However, the URL lists have duplicates, as two reviews can be for the same movie and thus be found on the same IMDB webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to save the URLs and their associated movies into a dictionary for later use. This way we can do all the scraping up front. Let's define a function which does this scraping for a given set of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url_dict(url_list):\n",
    "    '''\n",
    "    Input: List of URLs.\n",
    "    Output: Dictionary of URL: movie based on scraped movie title.\n",
    "    '''\n",
    "    url_dict = dict(zip(url_list, [None]*len(url_list)))\n",
    "    index = 0\n",
    "    for url in url_list:\n",
    "        if url_dict[url] == None:\n",
    "            url_dict[url] = get_movie(url)\n",
    "        # Every once in awhile, let us know how many URLs we have digested out of 12,500 total.\n",
    "        if random.random() < 0.001:\n",
    "            print index\n",
    "        index += 1\n",
    "        time.sleep(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a dictionary of stored movie names for each subdata set, saving into a JSON file so we only have to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "train_pos_dict = make_url_dict(train_pos_urls)\n",
    "fp = open(\"url_movie_train_pos.json\",\"w\")\n",
    "json.dump(train_pos_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did this right for training positives, the length of the dictionary keys should be equal to the number of unique URLs in its URL list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(train_pos_dict.keys()), len(list(set(list(train_pos_urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "train_neg_dict = make_url_dict(train_neg_urls)\n",
    "fp = open(\"url_movie_train_neg.json\",\"w\")\n",
    "json.dump(train_neg_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "test_pos_dict = make_url_dict(test_pos_urls)\n",
    "fp = open(\"url_movie_test_pos.json\",\"w\")\n",
    "json.dump(test_pos_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "test_neg_dict = make_url_dict(test_neg_urls)\n",
    "fp = open(\"url_movie_test_neg.json\",\"w\")\n",
    "json.dump(test_neg_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload\n",
    "with open(\"url_movie_tr_pos.json\", \"r\") as fd:\n",
    "    train_pos_dict = json.load(fd)\n",
    "with open(\"url_movie_train_neg.json\", \"r\") as fd:\n",
    "    train_neg_dict = json.load(fd)\n",
    "with open(\"url_movie_test_pos.json\", \"r\") as fd:\n",
    "    test_pos_dict = json.load(fd)\n",
    "with open(\"url_movie_test_neg.json\", \"r\") as fd:\n",
    "    test_neg_dict = json.load(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have saved movie names associated with each URL, we can finally create our data table of reviews. We will define a function `data_collect` which iterates over our directories, making a pandas dataframe out of all the reviews in a particular category (e.g. Test Set, Positive Reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_collect(directory, pos, url_dict, url_list):\n",
    "    '''\n",
    "    Inputs: \n",
    "        directory: Directory to collect reviews from. ex) 'aclImdb/train/pos/'\n",
    "        Pos: True or False, depending on whether the reviews are labelled positive or not.\n",
    "        url_dict: the relevant URL-Movie dictionary (created above) for the particular category\n",
    "        url_list: the list of URLs for that particular category\n",
    "    '''\n",
    "    # Column names for the data frame\n",
    "    review_df = pd.DataFrame(columns=['movie_id', 'stars', 'positive', 'text', 'url', 'movie_name'])\n",
    "    # Crawl over the directory, attaining relevant data for each of the .txt review files.\n",
    "    train_pos_names = list(os.walk(directory))[0][2]\n",
    "    for review in train_pos_names:\n",
    "        # Andrew L. Maas's stanford group encoded the reviewID and number of stars for a review in the file's name.\n",
    "        # For example, \"0_10.txt\" means reviewID 0 received 10 stars. The reviews are in the same order as the URLs,\n",
    "        # so the reviewID is precisely the location of that movie's URL in the respective URL list.\n",
    "        stars = int(review.split(\"_\")[1].split(\".\")[0])\n",
    "        movieID = int(review.split(\"_\")[0]) #everything before the underscore\n",
    "        fp = open('%(dir)s%(review)s' % {'dir': directory, 'review': review}, 'r')\n",
    "        text = fp.read()\n",
    "        url = url_list[movieID]\n",
    "        movie_name = url_dict[url]\n",
    "        reviewDict = {'movie_id': [movieID], 'stars': [stars], 'positive': [pos], 'text': [text], 'url': [url], 'movie_name': [movie_name]}\n",
    "        review_df = review_df.append(pd.DataFrame(reviewDict))\n",
    "    return review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to collect all our data. Let's first collect the training data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First get the positive reviews for the train_df.\n",
    "train_df = data_collect('aclImdb/train/pos/', True, train_pos_dict, train_pos_urls)\n",
    "# Then append the negative reviews\n",
    "train_df = train_df.append(data_collect('aclImdb/train/neg/', False, train_neg_dict, train_neg_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a testing data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First get the positive reviews for the train_df.\n",
    "test_df = data_collect('aclImdb/test/pos/', True, test_pos_dict, test_pos_urls)\n",
    "# Then append the negative reviews\n",
    "test_df = test_df.append(data_collect('aclImdb/test/neg/', False, test_neg_dict, test_neg_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary out of each dataframe so that we can save each in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_dict = {feature: train_df[feature].values.tolist() for feature in train_df.columns.values}\n",
    "test_df_dict = {feature: test_df[feature].values.tolist() for feature in test_df.columns.values}\n",
    "# Train\n",
    "fp = open(\"train_df_dict.json\",\"w\")\n",
    "json.dump(train_df_dict, fp)\n",
    "fp.close()\n",
    "# Test\n",
    "fp = open(\"test_df_dict.json\",\"w\")\n",
    "json.dump(test_df_dict, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reopen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"train_df_dict.json\", \"r\") as fd:\n",
    "    train_df_dict = json.load(fd)\n",
    "with open(\"test_df_dict.json\", \"r\") as fd:\n",
    "    test_df_dict = json.load(fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
